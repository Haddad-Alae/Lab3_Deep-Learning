Part 1
Le modèle LSTM montre une réduction de la perte au fil des époques, ce qui indique un apprentissage progressif. La perte passe de 35,76 à 22,47, ce qui montre que le modèle améliore ses prédictions. L'erreur quadratique moyenne (MSE) de 6,83 sur les données de test suggère que les prédictions ne sont pas parfaites

Part2 
À la suite de l'exécution de ce modèle GPT-2 fine-tuné, nous avons pu générer un texte cohérent à partir de la phrase d'entrée "Once upon a time". Le modèle a produit un texte narratif en continu, ce qui montre l'efficacité de l'apprentissage supervisé pour affiner un modèle préexistant sur un jeu de données personnalisé


Au cours de ce laboratoire, j'ai appris à utiliser des outils comme Scrapy et BeautifulSoup pour collecter des données textuelles, puis à appliquer des techniques de prétraitement du texte, telles que tokenisation, lemmatisation, et suppression des mots vides. J'ai ensuite entraîné plusieurs modèles de réseaux de neurones comme RNN, GRU, et LSTM pour la classification de texte, et j'ai ajusté leurs hyperparamètres pour optimiser les performances. Enfin, j'ai exploré l'utilisation du modèle GPT-2 pour la génération de texte en fine-tunant sur un jeu de données personnalisé.
