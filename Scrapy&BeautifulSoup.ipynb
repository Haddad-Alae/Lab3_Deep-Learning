{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2wIe82WitPm",
        "outputId": "d4402441-8aac-4922-b1d8-f710791c46af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                Text     Score\n",
            "0  وضع ناصر الخليفي رئيس نادي باريس سان جيرمان ال...  6.465954\n",
            "1  خسر النجم البرتغالي كريستيانو رونالدو لاعب الن...  6.036766\n",
            "2  تغطية مباشرة لمباراة الهلال السعودي ضد الغرافة...  6.521478\n",
            "3  تجاوز فريق برشلونة كبوته، محققا فوزا عريضا خار...  3.712635\n",
            "4  ردت اللجنة الأولمبية الدولية على احتمال تعارض ...  8.656187\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.isri import ISRIStemmer\n",
        "import pandas as pd\n",
        "import random\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "# Web Scraping with Error Handling\n",
        "def scrape_arabic_texts(urls):\n",
        "    texts = []\n",
        "    for url in urls:\n",
        "        try:\n",
        "            response = requests.get(url)\n",
        "            response.raise_for_status()  # Ensure the request was successful\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            # Adjust based on the site's structure\n",
        "            articles = soup.find_all('p')\n",
        "            for article in articles:\n",
        "                text = article.get_text(strip=True)\n",
        "                if text and len(text.split()) > 5:  # Filter very short texts\n",
        "                    texts.append(text)\n",
        "        except Exception as e:\n",
        "            print(f\"Error scraping {url}: {e}\")\n",
        "    return texts\n",
        "\n",
        "# Example websites\n",
        "urls = [\n",
        "    \"https://www.aljazeera.net/sport\",\n",
        "    \"https://arabic.cnn.com/sport\"\n",
        "]\n",
        "\n",
        "# Scrape Arabic texts\n",
        "texts = scrape_arabic_texts(urls)\n",
        "\n",
        "# Assign random relevance scores (replace with meaningful scoring if possible)\n",
        "scores = [random.uniform(0, 10) for _ in texts]\n",
        "\n",
        "# Create DataFrame\n",
        "dataset = pd.DataFrame({'Text': texts, 'Score': scores})\n",
        "print(dataset.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLZm0yQcj5oP",
        "outputId": "71748f8c-fdeb-4cb0-ff9e-6077a15cae48"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Arabic NLP Preprocessing Pipeline\n",
        "stemmer = ISRIStemmer()\n",
        "stop_words = set(stopwords.words(\"arabic\"))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "    # Normalize (remove diacritics and special characters)\n",
        "    tokens = [word for word in tokens if word.isalnum()]\n",
        "    # Remove stopwords\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    # Stemming\n",
        "    tokens = [stemmer.stem(word) for word in tokens]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# Apply preprocessing to the dataset\n",
        "dataset['Processed_Text'] = dataset['Text'].apply(preprocess_text)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(dataset['Processed_Text'], dataset['Score'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert text data to indices (simplified)\n",
        "vocab = {}\n",
        "def build_vocab(texts):\n",
        "    index = 0\n",
        "    for text in texts:\n",
        "        tokens = word_tokenize(text)\n",
        "        for token in tokens:\n",
        "            if token not in vocab:\n",
        "                vocab[token] = index\n",
        "                index += 1\n",
        "    vocab[\"<UNK>\"] = len(vocab)  # Add unknown token\n",
        "build_vocab(X_train)\n",
        "\n",
        "# Convert the texts into indices\n",
        "def text_to_indices(text, vocab):\n",
        "    tokens = word_tokenize(text)\n",
        "    return [vocab.get(token, vocab[\"<UNK>\"]) for token in tokens]\n",
        "\n",
        "X_train_indices = [text_to_indices(text, vocab) for text in X_train]\n",
        "X_test_indices = [text_to_indices(text, vocab) for text in X_test]\n",
        "\n",
        "# Pad sequences to have the same length (optional, here we use max length of training data)\n",
        "max_len = max([len(seq) for seq in X_train_indices])\n",
        "X_train_indices = [seq + [0] * (max_len - len(seq)) for seq in X_train_indices]\n",
        "X_test_indices = [seq + [0] * (max_len - len(seq)) for seq in X_test_indices]\n",
        "\n",
        "# Convert to tensors\n",
        "X_train_tensor = torch.tensor(X_train_indices, dtype=torch.long)\n",
        "X_test_tensor = torch.tensor(X_test_indices, dtype=torch.long)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
        "\n",
        "# Create DataLoader\n",
        "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_data = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n"
      ],
      "metadata": {
        "id": "HWBvgpPNjnYH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M5aGPIF2kBtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define different RNN architectures\n",
        "\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.rnn = nn.RNN(hidden_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        rnn_out, _ = self.rnn(embedded)\n",
        "        out = self.fc(rnn_out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "class BidirectionalRNNModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
        "        super(BidirectionalRNNModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.rnn = nn.RNN(hidden_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(2 * hidden_size, output_size)  # Multiply by 2 for bidirectional\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        rnn_out, _ = self.rnn(embedded)\n",
        "        out = self.fc(rnn_out[:, -1, :])\n",
        "        return out"
      ],
      "metadata": {
        "id": "3dnkc1gkj0aN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
        "        super(GRUModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        gru_out, _ = self.gru(embedded)\n",
        "        out = self.fc(gru_out[:, -1, :])\n",
        "        return out"
      ],
      "metadata": {
        "id": "73oHTdPIkCUb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(embedded)\n",
        "        out = self.fc(lstm_out[:, -1, :])\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "Rrl4L3-EkEWU"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Choose model type (You can change this to 'RNN', 'BidirectionalRNN', 'GRU', or 'LSTM')\n",
        "model_type = 'LSTM'  # You can also try 'RNN', 'BidirectionalRNN', 'GRU'\n",
        "\n",
        "# Instantiate the selected model\n",
        "if model_type == 'RNN':\n",
        "    model = RNNModel(input_size=len(vocab), hidden_size=128, output_size=1)\n",
        "elif model_type == 'BidirectionalRNN':\n",
        "    model = BidirectionalRNNModel(input_size=len(vocab), hidden_size=128, output_size=1)\n",
        "elif model_type == 'GRU':\n",
        "    model = GRUModel(input_size=len(vocab), hidden_size=128, output_size=1)\n",
        "elif model_type == 'LSTM':\n",
        "    model = LSTMModel(input_size=len(vocab), hidden_size=128, output_size=1)\n",
        "\n",
        "# Define loss and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Function to compute accuracy (based on absolute error)\n",
        "def compute_accuracy(y_pred, y_true, threshold=1.0):\n",
        "    abs_error = torch.abs(y_pred - y_true)\n",
        "    correct = (abs_error <= threshold).float()\n",
        "    accuracy = correct.sum() / correct.size(0)\n",
        "    return accuracy\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for inputs, labels in train_loader:\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.squeeze(), labels)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
        "\n",
        "# Evaluate the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "    accuracies = []\n",
        "    for inputs, labels in test_loader:\n",
        "        outputs = model(inputs)\n",
        "        y_pred.extend(outputs.squeeze().tolist())\n",
        "        y_true.extend(labels.tolist())\n",
        "\n",
        "        # Compute accuracy for the batch\n",
        "        batch_accuracy = compute_accuracy(outputs.squeeze(), labels)\n",
        "        accuracies.append(batch_accuracy.item())\n",
        "\n",
        "# Calculate MSE and accuracy on test data\n",
        "mse = criterion(torch.tensor(y_pred), torch.tensor(y_true))\n",
        "accuracy = sum(accuracies) / len(accuracies)\n",
        "\n",
        "print(f\"Test MSE: {mse.item()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DLNB3VhkHo5",
        "outputId": "e40b08e1-ee4a-44d1-cc1a-7e5e0657ee1a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Loss: 35.75990295410156\n",
            "Epoch 2/5, Loss: 31.750770568847656\n",
            "Epoch 3/5, Loss: 28.2264347076416\n",
            "Epoch 4/5, Loss: 25.14720916748047\n",
            "Epoch 5/5, Loss: 22.46776008605957\n",
            "Test MSE: 6.828156471252441\n"
          ]
        }
      ]
    }
  ]
}